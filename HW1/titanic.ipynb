{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_dataset(X, train=False, mean=0, std=1):\n",
    "    \"\"\"\n",
    "    Preprocess the Titanic dataset by dropping irrelevant features, encoding categorical variables, \n",
    "    handling missing values, and applying feature scaling.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): The input dataset to preprocess.\n",
    "        train (bool): Indicates whether the dataset is the training set. If True, calculates and returns\n",
    "                      the mean and standard deviation for scaling. If False, applies the provided mean and std.\n",
    "        mean (pd.Series or float): Mean of the training set for scaling (used when train=False).\n",
    "        std (pd.Series or float): Standard deviation of the training set for scaling (used when train=False).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed dataset.\n",
    "        pd.Series: Mean of the features (only returned if train=True).\n",
    "        pd.Series: Standard deviation of the features (only returned if train=True).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Drop columns that are irrelevant or not useful for model training\n",
    "    X = X.drop([\"Survived\", \"PassengerId\", \"Name\", \"Embarked\", \"Ticket\", \"Cabin\"], axis=1, errors=\"ignore\")\n",
    "\n",
    "    # Encode 'Sex' column as binary: male -> 1, female -> 0\n",
    "    X[\"Sex\"] = X[\"Sex\"].apply(lambda x: 1 if x == \"male\" else 0)\n",
    "\n",
    "    # Fill missing values with the mean of each column\n",
    "    for col in X.columns:\n",
    "        X[col] = X[col].fillna(X[col].mean())\n",
    "\n",
    "    # Add a new binary feature 'Child' where Age < 15 -> 1 (child), otherwise -> 0\n",
    "    X[\"Child\"] = X[\"Age\"].apply(lambda x: 1 if x < 15 else 0)\n",
    "\n",
    "    # Add a new binary feature 'Cheap_Fare' where Fare < 18 -> 1 (cheap), otherwise -> 0\n",
    "    X[\"Cheap_Fare\"] = X[\"Fare\"].apply(lambda x: 1 if x < 18 else 0)\n",
    "\n",
    "    # If the dataset is for training, compute the mean and standard deviation for scaling\n",
    "    if train:\n",
    "        mean = X.mean()\n",
    "        std = X.std()\n",
    "        X = (X - mean) / std  # Normalize the features\n",
    "    else:\n",
    "        # Apply the mean and standard deviation from the training set\n",
    "        X = (X - mean) / std\n",
    "\n",
    "    # Create a new feature 'Family' that sums 'SibSp' (siblings/spouses) and 'Parch' (parents/children)\n",
    "    X[\"Family\"] = X[\"SibSp\"] + X[\"Parch\"]\n",
    "\n",
    "    # Drop columns no longer needed after feature engineering\n",
    "    X = X.drop([\"Age\", \"Fare\", \"SibSp\", \"Parch\"], axis=1)\n",
    "\n",
    "    # Return the dataset and mean/std if training, otherwise just the dataset\n",
    "    if train:\n",
    "        return X, mean, std\n",
    "    return X\n",
    "\n",
    "def load_datasets():\n",
    "    \"\"\"\n",
    "    Load and preprocess the Titanic dataset, separating the features and labels for training and testing.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed training features.\n",
    "        pd.Series: Training labels (Survived column).\n",
    "        pd.DataFrame: Preprocessed testing features.\n",
    "        pd.Series: Testing labels (Survived column).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the train and test datasets from CSV files\n",
    "    x_train = pd.read_csv(os.path.join(\"titanic_data\", \"train.csv\"))\n",
    "    x_test = pd.read_csv(os.path.join(\"titanic_data\", \"test.csv\"))\n",
    "    \n",
    "    # Extract the target variable 'Survived' for the training dataset\n",
    "    y_train = x_train[\"Survived\"]\n",
    "    \n",
    "    # Load the test set labels from a separate submission CSV\n",
    "    y_test = pd.read_csv(os.path.join(\"titanic_data\", \"gender_submission.csv\"))[\"Survived\"]\n",
    "    \n",
    "    # Preprocess the training dataset and retrieve the mean and std for normalization\n",
    "    x_train, mean, std = prepare_dataset(x_train, train=True)\n",
    "    \n",
    "    # Preprocess the test dataset using the same mean and std as the training set\n",
    "    x_test = prepare_dataset(x_test, mean=mean, std=std)\n",
    "\n",
    "    # Return the preprocessed features and labels for both training and testing\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# Load and preprocess the datasets\n",
    "x_train, y_train, x_test, y_test = load_datasets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection For Titanic Dataset\n",
    "\n",
    "For selecting features to train our model with for the titanic dataset, we first did a preliminary filter on features that we believed wouldn't be useful in determing whether or not a passenger has survived. We first removed categorical features such as Names and Cabin because we couldn't find a good and meaningful way to generate categories from them that would help in classifying the passengers. We had tried to parse through the names and look for titles such as Mrs to see whether or not a female passenger was married but this didn't seem to help much. We then removed ticket number and passenger ID which didn't seem to have any influence on the passenger's chance of survival. We also originally had planned to keep the Embarked feature but we found that after one hot encoding, our performance on the test set dropped by around 4-5% and we believe that this had occurred because our model had overfit on the train set because the one hot encoding had introduced too many new features.\n",
    "\n",
    "We kept the following attributes to use to train our model: PClass, Sex, Age, SibSp, Parch, and Fare. We kept the class of the passenger as we believed that it was likely that unfortunately people with higher classes during the crash were more likely to be offered the raft and therefore survive the crash. We chose sex because we found that generally women and children were first given priority to go on the raft. Because of this, we also created a Child feature which is a categorical feature that is 1 if the passenger is younger than 15 and 0 otherwise. The idea behind this, is that by creating a discrete feature that directly tells the model whether or not a passenger was a child, we hoped that the model would have an easier time picking up on the relationship between age and a passenger's chance of survival. After this, we dropped the Age feature because we found that the survival rate of passengers older than 15 was roughly the same meaning that the Age feature couldn't provide us with any more discrimatory information.\n",
    "\n",
    "The SibSp and Parch represent the number of siblings and parents or children that a passenger had on board wth them respectively. We believed that this would be incredibly useful information to the model, in particular, because we could create a custom feature using these attributes called Family which gives the number of family members that a passenger had on board. The idea is that if a person had family members on board, then they might be more likely to advocate for their family members to go on the raft instead of them. After doing this, we dropped the SibSp and Parch class since we believed that with the Family feature, we had extracted the bulk of information that these two features collectively had to offer.\n",
    "\n",
    "Our last feature that we used was the ticket fare. Here, we found the passengers that had paid more for their tickets had a greater chance of survival. This likely has to do with the fact that if a passenger had bought a more expensive ticket then they likely are in a higher class which we believe correlates with a higher survival rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8103254769921436\n",
      "0.9569377990430622\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create an instance of the LogisticRegression model and fit it to the training data\n",
    "clf = LogisticRegression().fit(x_train, y_train)\n",
    "\n",
    "# Print the accuracy score of the model on the training set\n",
    "print(clf.score(x_train, y_train))\n",
    "\n",
    "# Print the accuracy score of the model on the test set\n",
    "print(clf.score(x_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
